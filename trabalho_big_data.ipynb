{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-7onfLR9pIm"
      },
      "source": [
        "# Comandos para realiza√ß√£o do trabalho da mat√©ria de Big Data com uso da biblioteca PySpark.\n",
        "\n",
        "Este notebook foi projetado para guiar os alunos na realiza√ß√£o das pr√°ticas de Big Data utilizando PySpark. Certifique-se de seguir cada etapa cuidadosamente para garantir a correta execu√ß√£o das atividades.\n",
        "\n",
        "Seu trabalho come√ßar√° na c√©lula 5. Execute as 4 primeiras c√©lulas para iniciar a atividade.\n",
        "\n",
        "## <font color=red>Observa√ß√£o importante:</font>\n",
        "\n",
        "<font color=yellow>Trabalho realizado com uso da biblioteca pandas n√£o ser√° aceito!</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2FCSMi19pIn"
      },
      "source": [
        "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab\n",
        "\n",
        "Aqui, voc√™ far√° o download do dataset necess√°rio para as atividades. Certifique-se de que o arquivo foi descompactado corretamente antes de prosseguir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuvS_bhS9pIo",
        "outputId": "5b62e362-e358-4545-dbd5-2c911af89634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded imdb-reviews-pt-br.zip successfully!\n",
            "Extracted files from imdb-reviews-pt-br.zip\n",
            "Removed imdb-reviews-pt-br.zip\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\"\n",
        "filename = \"imdb-reviews-pt-br.zip\"\n",
        "\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    with open(filename, \"wb\") as f:\n",
        "        for chunk in response.iter_content(1024):\n",
        "            f.write(chunk)\n",
        "    print(f\"Downloaded {filename} successfully!\")\n",
        "else:\n",
        "    print(f\"Error downloading file: {response.status_code}\")\n",
        "\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "    print(\"Extracted files from\", filename)\n",
        "\n",
        "os.remove(filename)\n",
        "print(f\"Removed {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQc1AM559pIp"
      },
      "source": [
        "## Instala√ß√£o manual das depend√™ncias para uso do pyspark no Google Colab\n",
        "\n",
        "Esta etapa garante que todas as bibliotecas necess√°rias para o PySpark sejam instaladas no Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPh3lnk59pIp"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6JqpYgG9pIp"
      },
      "source": [
        "## Importar, instanciar e criar a SparkSession\n",
        "\n",
        "A SparkSession √© o ponto de entrada para usar o PySpark. Certifique-se de configurar corretamente o nome do aplicativo e o master."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSbiGG8H9pIq",
        "outputId": "10063124-21d4-42a2-d407-675ecf75ffa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando spark session para Ezequiel - RU 4467541\n",
            "Sess√£o iniciado com sucesso! üöÄ\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "MEU_RU = \"4467541\"\n",
        "appName = f\"PySpark Trabalho de Big Data - {MEU_RU}\"\n",
        "master = \"local\"\n",
        "\n",
        "print(f\"Iniciando spark session para Ezequiel - RU {MEU_RU}\")\n",
        "spark: SparkSession = SparkSession.builder.appName(appName).master(master).getOrCreate()\n",
        "print(\"Sess√£o iniciado com sucesso! üöÄ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD043mdq9pIr"
      },
      "source": [
        "## Criar spark dataframe do CSV utilizando o m√©todo read.csv do spark\n",
        "\n",
        "N√£o altere este c√≥digo e use o dataframe imdb_df criado aqui em todo o seu trabalho. A cria√ß√£o de um dataframe diferente deste poder√° causar erros na coluna sentiment e isso refletir√° em erros de resposta das quest√µes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vjEBgCc79pIr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "schema = StructType([\n",
        "  StructField(\"id\", StringType(), True),\n",
        "  StructField(\"text_en\", StringType(), True),\n",
        "  StructField(\"text_pt\", StringType(), True),\n",
        "  StructField(\"sentiment\", StringType(), True),\n",
        "])\n",
        "\n",
        "\n",
        "imdb_df: DataFrame = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NTFRMQB9pIs"
      },
      "source": [
        "# Quest√£o 1\n",
        "\n",
        "Nesta quest√£o, voc√™ ir√° calcular a soma dos IDs para entradas onde o sentimento ('sentiment') √© 'neg'.\n",
        "\n",
        "### Objetivo:\n",
        "- Usar a coluna 'sentiment' como chave e somar os valores da coluna 'id'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKzimkl49pIs"
      },
      "source": [
        "## Criar fun√ß√µes de MAP:\n",
        "- Criar fun√ß√£o para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro\n",
        "\n",
        "A fun√ß√£o map ir√° transformar cada linha do dataframe em uma **tupla** (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: coluna 'id' convertida para inteiro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kVVfKKA_9pIs"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "\n",
        "def filter_negative_reviews(data: DataFrame) -> DataFrame:\n",
        "\n",
        "    return data.filter(data[\"sentiment\"] == \"neg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq49YB_w9pIt"
      },
      "source": [
        "## Cria fun√ß√µes de REDUCE:\n",
        "\n",
        "- Criar fun√ß√£o de reduce para somar os IDs por \"sentiment\".\n",
        "\n",
        "A fun√ß√£o reduce ir√° somar os valores dos IDs agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x8UBSDrW9pIt"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "def sum_negative_ids(reviews: DataFrame) -> DataFrame:\n",
        "\n",
        "  return reviews.withColumn(\"id\", col(\"id\").cast(\"int\")).select(sum(\"id\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZaCarj19pIt"
      },
      "source": [
        "## Aplica√ß√£o do map/reduce e visualiza√ß√£o do resultado\n",
        "\n",
        "Aqui, voc√™ aplicar√° as fun√ß√µes de map e reduce ao dataframe Spark para calcular os resultados. N√£o se esque√ßa de usar o m√©todo `.collect()` para visualizar os resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s3x1ohf9pIt",
        "outputId": "08e3bf2c-5eb8-4c80-8502-7006eb1bfd2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome √© Ezequiel √© meu RU √© 4467541\n",
            "Soma de IDs das reviews negativas: 459568555\n"
          ]
        }
      ],
      "source": [
        "negative_reviews = filter_negative_reviews(imdb_df)\n",
        "sum_of_negative_ids = sum_negative_ids(negative_reviews).collect()[0][0]\n",
        "\n",
        "print(f\"Meu nome √© Ezequiel √© meu RU √© 4467541\")\n",
        "print(f\"Soma de IDs das reviews negativas: {sum_of_negative_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gVg4MNt9pIt"
      },
      "source": [
        "# Quest√£o 2:\n",
        "\n",
        "Nesta quest√£o, voc√™ ir√° calcular a diferen√ßa no n√∫mero total de palavras entre textos negativos em portugu√™s e ingl√™s.\n",
        "\n",
        "### Objetivo:\n",
        "- Contar as palavras em cada idioma (colunas 'text_pt' e 'text_en') para entradas onde o sentimento ('sentiment') √© 'neg'.\n",
        "- Subtrair o total de palavras em ingl√™s do total em portugu√™s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epto4pId9pIt"
      },
      "source": [
        "## Criar fun√ß√µes de MAP:\n",
        "- Criar fun√ß√£o para mapear o \"sentiment\" como chave de uma tupla principal e como valor uma outra tupla com a soma das palavras de cada idioma como valor.\n",
        "\n",
        "A fun√ß√£o map ir√° transformar cada linha do dataframe em uma tupla (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: Nova tupla com:\n",
        "  - Elemento 0: soma das palavras da coluna 'text_en'\n",
        "  - Elemento 1: soma das palavras da coluna 'text_pt'\n",
        "\n",
        "OU\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: (soma das palavras da coluna 'text_pt') - (soma das palavras da coluna 'text_en')\n",
        "  \n",
        "\n",
        "Para contar as palavras deve-se primeiro separar os textos em uma lista de palavras para ent√£o descobrir o tamanho desta lista.\n",
        "Dicas:\n",
        "\n",
        "1. Use o m√©todo .split() e n√£o .split(\" \") de string para separar as palavras em uma lista ou use a fun√ß√£o split(coluna de texto, regex) do pyspark com o regex igual √† \"[ ]+\" ou \"\\s+\"\n",
        "2. Use len() para descobrir o tamanho da lista de palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1EEBKOI19pIu"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col, split, size\n",
        "\n",
        "def map_sentiment_to_word_count(data: DataFrame) -> DataFrame:\n",
        "\n",
        "  return data.select(col(\"sentiment\"), size(split(col(\"text_en\"), \"\\\\s+\")).alias(\"text_en_word_count\"), size(split(col(\"text_pt\"), \"\\\\s+\")).alias(\"text_pt_word_count\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqE26wMs9pIu"
      },
      "source": [
        "## Cria fun√ß√µes de REDUCE:\n",
        "\n",
        "- Criar fun√ß√£o de reduce para somar o numero de palavras de cada texto portugu√™s e ingl√™s por \"sentiment\" (depender√° de como voc√™ optou por fazer sua fun√ß√£o map2).\n",
        "\n",
        "A fun√ß√£o reduce ir√° somar os valores das quantidades de palavras agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DnvcShSx9pIu"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "\n",
        "def reduce_word_count_by_sentiment(sentiment_word_counts: DataFrame) -> DataFrame:\n",
        "\n",
        "  return sentiment_word_counts.groupBy(\"sentiment\").agg(sum(\"text_en_word_count\").alias(\"total_text_en_words\"), sum(\"text_pt_word_count\").alias(\"total_text_pt_words\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwGG68KT9pIu"
      },
      "source": [
        "## Aplica√ß√£o do map/reduce e visualiza√ß√£o do resultado\n",
        "\n",
        "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
        "2. Selecionar os dados referentes aos textos negativos para realizar a subtra√ß√£o.\n",
        "3. Realizar a subtra√ß√£o das contagens de palavras dos textos negativos para obter o resultado final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AahQqw6y9pIu",
        "outputId": "b445f231-305f-4179-e049-aec1a2f45032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome √© Ezequiel √© meu RU √© 4467541\n",
            "Diferen√ßa entre a contagem de palavras: 54976 (Texto em PT - Texto em EN)\n"
          ]
        }
      ],
      "source": [
        "negative_data = filter_negative_reviews(imdb_df)\n",
        "\n",
        "sentiment_word_counts = map_sentiment_to_word_count(negative_data)\n",
        "total_word_counts = reduce_word_count_by_sentiment(sentiment_word_counts)\n",
        "result = total_word_counts.collect()[0]\n",
        "word_count_difference = result[2] - result[1]\n",
        "\n",
        "print(f\"Meu nome √© Ezequiel √© meu RU √© 4467541\")\n",
        "print(f\"Diferen√ßa entre a contagem de palavras: {word_count_difference} (Texto em PT - Texto em EN)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}